from Data.DataLoader import make_dataloader
from src.Loss import MainLoss, KLBetaAnnealing
from src.Layers import Model
from Data import DataLoader
import numpy as np
import torch
import matplotlib.pyplot as plt
import os

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

CURRENT_PATH = os.path.abspath(__file__)
PATH = os.path.join(os.path.dirname(os.path.dirname(CURRENT_PATH)), "models/model.pth")

#Class to implement the early stopping mecahnism
class EarlyStopping:

    def __init__(self, patience):

        self.patience = patience
        self.counter = 0
        self.minimum = 0

    def step(self, loss):

        if loss < self.minimum:
            self.counter += 1
        else:
            self.minimum = loss
            self.counter = 0

        if self.counter >= self.patience:
            return True
        else:
            return False

#Main training loop function
def train_model(model, optimizer, loss_fn, train_loader, val_loader, epochs, scheduler, early_stopper, beta_annealer):

    for epoch in range(epochs):

        model.train()

        print("Epoch " + str(epoch))

        # Training batch loop
        loss_sum = 0
        beta = beta_annealer.step()

        for batch_idx, data in enumerate(train_loader):

            generated, mean, z, variance  = model(data)

            loss = loss_fn(generated, data, mean, z, variance, beta)
            loss.backward()
            loss_sum += loss.item()

            optimizer.step()
            optimizer.zero_grad()

            if batch_idx % 45 == 0 and epoch % 10 == 0:
                plt.imshow((generated[0].detach().numpy().transpose(1, 2, 0)))
                plt.show()
                plt.imshow(data[0].detach().numpy().transpose(1, 2, 0))
                plt.show()

            print("     At Train Batch " + str(batch_idx) + " Loss: " + str(loss.item()))

        #Validation loop
        model.eval()

        with torch.no_grad():

            val_loss_sum = 0

            for batch_idx, data in enumerate(val_loader):

                generated, mean, z, variance = model(data)
                loss = loss_fn(generated, data, mean, z, variance, beta)
                val_loss_sum += loss.item()

                print("     At Validation Batch " + str(batch_idx) + " Loss: " + str(loss.item()))

                if batch_idx % 20 == 1 and epoch % 10 == 0:
                        plt.imshow((generated[0].detach().numpy().transpose(1, 2, 0)))
                        plt.show()
                        plt.imshow(data[0].detach().numpy().transpose(1, 2, 0))
                        plt.show()

            val_loss_sum /= 79
            loss_sum /= 395

        checkpoint = {
            'model' : model.state_dict(),
            'optimizer' : optimizer.state_dict(),
            'epoch' : epoch,
            'val_loss' : val_loss_sum,
            'train_loss' : loss_sum,
        }

        torch.save(checkpoint, PATH)

        #Scheduler and early stop updates
        scheduler.step(val_loss_sum)
        stop = early_stopper.step(val_loss_sum)

        if stop:
            break

        print('Overall Train loss after Epoch ' + str(epoch) + ' is ' + str(loss_sum))
        print('Overall Val Loss after Epoch ' + str(epoch) + ' is ' + str(val_loss_sum))

#Intializing required objects
model = Model()
Optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(Optimizer, patience=3, cooldown=2, factor=0.8)
early_stopper = EarlyStopping(10)
beta_annealer = KLBetaAnnealing(cycle = 6, max_beta = 0.5)
loss_fn = MainLoss()
train, test = DataLoader.load_all_data()
train_loader = make_dataloader(train)
val_loader = make_dataloader(test)
epochs = 200

#calling the training loop
train_model(model, Optimizer, loss_fn, train_loader, val_loader, epochs, scheduler, early_stopper, beta_annealer)

# viewing the images generated by the model
for batch_idx, data in enumerate(val_loader):
    generated, z, variance = model(data)
    for i in range(len(generated)):
        plt.imshow((generated[i].detach().numpy().transpose(1, 2, 0)))
        plt.show()
        plt.imshow(data[i].detach().numpy().transpose(1, 2, 0))
        plt.show()